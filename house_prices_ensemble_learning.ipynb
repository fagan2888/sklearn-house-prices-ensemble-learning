{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "In statistics and machine learning, **ensemble methods** use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Simply put,\n",
    "\n",
    "1. Ensemble models in machine learning combine the decisions from multiple models to improve the overall performance.\n",
    "\n",
    "\n",
    "2. The main causes of error in learning models are due to noise, bias and variance. Ensemble methods help to minimize these factors.\n",
    "\n",
    "\n",
    "3. Ensembles are a divide and conquer approach used to improve performance.\n",
    "\n",
    "Ensemble learning can help you to build a really robust model from a few \"weak\" models, which eliminates a lot of the model tuning that would else be needed to achieve good results. They are especially popular in data science competitions because for these competitions the highest accuracy is more important than the runtime or interpretability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import xgboost as xgb\n",
    "# import lightgbm as lgb\n",
    "from statistics import mode\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingClassifier, AdaBoostRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone, RegressorMixin\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices\n",
    "\n",
    "In this notebook, we will go through the most common ensembling techniques out there. We will work on the [House Prices Regression dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) which can be freely downloaded from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path(\"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the train and test dataset\n",
    "train = pd.read_csv(path/'train.csv')\n",
    "test = pd.read_csv(path/'test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoolQC           1453\n",
       "MiscFeature      1406\n",
       "Alley            1369\n",
       "Fence            1179\n",
       "FireplaceQu       690\n",
       "LotFrontage       259\n",
       "GarageCond         81\n",
       "GarageType         81\n",
       "GarageYrBlt        81\n",
       "GarageFinish       81\n",
       "GarageQual         81\n",
       "BsmtExposure       38\n",
       "BsmtFinType2       38\n",
       "BsmtFinType1       37\n",
       "BsmtCond           37\n",
       "BsmtQual           37\n",
       "MasVnrArea          8\n",
       "MasVnrType          8\n",
       "Electrical          1\n",
       "Utilities           0\n",
       "YearRemodAdd        0\n",
       "MSSubClass          0\n",
       "Foundation          0\n",
       "ExterCond           0\n",
       "ExterQual           0\n",
       "Exterior2nd         0\n",
       "Exterior1st         0\n",
       "RoofMatl            0\n",
       "RoofStyle           0\n",
       "YearBuilt           0\n",
       "                 ... \n",
       "GarageArea          0\n",
       "PavedDrive          0\n",
       "WoodDeckSF          0\n",
       "OpenPorchSF         0\n",
       "3SsnPorch           0\n",
       "BsmtUnfSF           0\n",
       "ScreenPorch         0\n",
       "PoolArea            0\n",
       "MiscVal             0\n",
       "MoSold              0\n",
       "YrSold              0\n",
       "SaleType            0\n",
       "Functional          0\n",
       "TotRmsAbvGrd        0\n",
       "KitchenQual         0\n",
       "KitchenAbvGr        0\n",
       "BedroomAbvGr        0\n",
       "HalfBath            0\n",
       "FullBath            0\n",
       "BsmtHalfBath        0\n",
       "BsmtFullBath        0\n",
       "GrLivArea           0\n",
       "LowQualFinSF        0\n",
       "2ndFlrSF            0\n",
       "1stFlrSF            0\n",
       "CentralAir          0\n",
       "SaleCondition       0\n",
       "Heating             0\n",
       "TotalBsmtSF         0\n",
       "Id                  0\n",
       "Length: 81, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "# Drop id column because is is unnecessary for making predictions\n",
    "train.drop('Id', axis=1, inplace=True)\n",
    "test.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Feature engineering is important for traditional machine learning, but it's not the focus of this project. You can take reference of this [Stacked Regressions to Predict House Prices](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard) for data exploration on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice'] = np.log1p(train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data size is: (2919, 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z00045494\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "y_train = train.SalePrice.values\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "print('all_data size is: {}'.format(all_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PoolQC</th>\n",
       "      <td>99.657417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MiscFeature</th>\n",
       "      <td>96.402878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alley</th>\n",
       "      <td>93.216855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fence</th>\n",
       "      <td>80.438506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FireplaceQu</th>\n",
       "      <td>48.646797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LotFrontage</th>\n",
       "      <td>16.649538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageQual</th>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageCond</th>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageFinish</th>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <td>5.447071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GarageType</th>\n",
       "      <td>5.378554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtExposure</th>\n",
       "      <td>2.809181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtCond</th>\n",
       "      <td>2.809181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtQual</th>\n",
       "      <td>2.774923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <td>2.740665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <td>2.706406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrType</th>\n",
       "      <td>0.822199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MasVnrArea</th>\n",
       "      <td>0.787941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSZoning</th>\n",
       "      <td>0.137033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <td>0.068517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Missing Ration\n",
       "PoolQC             99.657417\n",
       "MiscFeature        96.402878\n",
       "Alley              93.216855\n",
       "Fence              80.438506\n",
       "FireplaceQu        48.646797\n",
       "LotFrontage        16.649538\n",
       "GarageQual          5.447071\n",
       "GarageCond          5.447071\n",
       "GarageFinish        5.447071\n",
       "GarageYrBlt         5.447071\n",
       "GarageType          5.378554\n",
       "BsmtExposure        2.809181\n",
       "BsmtCond            2.809181\n",
       "BsmtQual            2.774923\n",
       "BsmtFinType2        2.740665\n",
       "BsmtFinType1        2.706406\n",
       "MasVnrType          0.822199\n",
       "MasVnrArea          0.787941\n",
       "MSZoning            0.137033\n",
       "BsmtFullBath        0.068517"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_na = (all_data.isnull().sum() / len(all_data))*100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na==0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ration': all_data_na})\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['PoolQC'] = all_data['PoolQC'].fillna('None')\n",
    "all_data['MiscFeature'] = all_data['MiscFeature'].fillna('None')\n",
    "all_data['Alley'] = all_data['Alley'].fillna('None')\n",
    "all_data['Fence'] = all_data['Fence'].fillna('None')\n",
    "all_data['FireplaceQu'] = all_data['FireplaceQu'].fillna('None')\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "all_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\n",
    "all_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)\n",
    "all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n",
    "all_data = all_data.drop(['Utilities'], axis=1)\n",
    "all_data['Functional'] = all_data['Functional'].fillna('Typ')\n",
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n",
    "all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\n",
    "all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n",
    "all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Ratio]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_na = (all_data.isnull().sum() / len(all_data))*100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na==0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio':all_data_na})\n",
    "missing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n",
    "train.to_csv(path/'train_cleaned.csv')\n",
    "test.to_csv(path/'test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Numerical Features to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "\n",
    "all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape all_data (2919, 78)\n"
     ]
    }
   ],
   "source": [
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(all_data[c].values))\n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))\n",
    "    \n",
    "print('Shape all_data {}'.format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skew in numeric features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MiscVal</th>\n",
       "      <td>21.947195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoolArea</th>\n",
       "      <td>16.898328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LotArea</th>\n",
       "      <td>12.822431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <td>12.088761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3SsnPorch</th>\n",
       "      <td>11.376065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LandSlope</th>\n",
       "      <td>4.975157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <td>4.302254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <td>4.146143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <td>4.003891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ScreenPorch</th>\n",
       "      <td>3.946694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Skew\n",
       "MiscVal        21.947195\n",
       "PoolArea       16.898328\n",
       "LotArea        12.822431\n",
       "LowQualFinSF   12.088761\n",
       "3SsnPorch      11.376065\n",
       "LandSlope       4.975157\n",
       "KitchenAbvGr    4.302254\n",
       "BsmtFinSF2      4.146143\n",
       "EnclosedPorch   4.003891\n",
       "ScreenPorch     3.946694"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_feats = all_data.dtypes[all_data.dtypes!='object'].index\n",
    "\n",
    "# Check the skew of all the numeric features\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print('Skew in numeric features:')\n",
    "skewness = pd.DataFrame({'Skew': skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 59 skewed numerical features to Box Cox transform\n"
     ]
    }
   ],
   "source": [
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print('There are {} skewed numerical features to Box Cox transform'.format(skewness.shape[0]))\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2919, 79)\n",
      "(2919, 221)\n"
     ]
    }
   ],
   "source": [
    "print(all_data.shape)\n",
    "all_data = pd.get_dummies(all_data)\n",
    "print(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>Alley</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleType_COD</th>\n",
       "      <th>SaleType_CWD</th>\n",
       "      <th>SaleType_Con</th>\n",
       "      <th>SaleType_ConLD</th>\n",
       "      <th>SaleType_ConLI</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.692623</td>\n",
       "      <td>11.686189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>11.170327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.792276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>12.062832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.892039</td>\n",
       "      <td>11.724598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>10.200343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.013683</td>\n",
       "      <td>11.354094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.540963</td>\n",
       "      <td>8.274266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.510588</td>\n",
       "      <td>12.271365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730463</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>1.820334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.971129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.194318</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    1stFlrSF   2ndFlrSF  3SsnPorch     Alley  BedroomAbvGr  BsmtCond  \\\n",
       "0  11.692623  11.686189        0.0  0.730463      1.540963  1.820334   \n",
       "1  12.792276   0.000000        0.0  0.730463      1.540963  1.820334   \n",
       "2  11.892039  11.724598        0.0  0.730463      1.540963  1.820334   \n",
       "3  12.013683  11.354094        0.0  0.730463      1.540963  0.730463   \n",
       "4  12.510588  12.271365        0.0  0.730463      1.820334  1.820334   \n",
       "\n",
       "   BsmtExposure  BsmtFinSF1  BsmtFinSF2  BsmtFinType1  ...  \\\n",
       "0      1.540963   11.170327         0.0      1.194318  ...   \n",
       "1      0.730463   12.062832         0.0      0.000000  ...   \n",
       "2      1.194318   10.200343         0.0      1.194318  ...   \n",
       "3      1.540963    8.274266         0.0      0.000000  ...   \n",
       "4      0.000000   10.971129         0.0      1.194318  ...   \n",
       "\n",
       "   SaleCondition_Partial  SaleType_COD  SaleType_CWD  SaleType_Con  \\\n",
       "0                      0             0             0             0   \n",
       "1                      0             0             0             0   \n",
       "2                      0             0             0             0   \n",
       "3                      0             0             0             0   \n",
       "4                      0             0             0             0   \n",
       "\n",
       "   SaleType_ConLD  SaleType_ConLI  SaleType_ConLw  SaleType_New  SaleType_Oth  \\\n",
       "0               0               0               0             0             0   \n",
       "1               0               0               0             0             0   \n",
       "2               0               0               0             0             0   \n",
       "3               0               0               0             0             0   \n",
       "4               0               0               0             0             0   \n",
       "\n",
       "   SaleType_WD  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "\n",
       "[5 rows x 221 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('float64'), dtype('uint8')], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.24769912, 12.10901644, 12.31717117, ..., 12.49313327,\n",
       "       11.86446927, 11.90159023])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "def get_cv_scores(model, X, y, print_scores=True):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X)\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    if print_scores:\n",
    "        print(f'Root mean squared error: {rmse.mean():.3f} ({rmse.std():.3f})')\n",
    "    return [rmse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Models\n",
    "Before we can start working through the different ensembling techniques we need to define some base models which will be used for ensembling. For this data-set, we will use a Lasso regression, Gradient Boosting, XGBoost, and lightGBM model.\n",
    "\n",
    "We will validate the results using the root mean squared error and cross-validation.\n",
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error: 0.124 (0.016)\n",
      "Wall time: 628 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.1036135 , 0.13480558, 0.12784684, 0.10698948, 0.14677378])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_cv_scores(lasso_model, X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.23428681])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_model.fit(X, y_train)\n",
    "lasso_model.predict([X[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.24769911637256"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285256686605036"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_model.score(X, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error: 0.210 (0.024)\n",
      "Wall time: 284 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.18915863, 0.24631617, 0.201391  , 0.18288652, 0.23084938])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dt = DecisionTreeRegressor()\n",
    "get_cv_scores(dt, X, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z00045494\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\z00045494\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\z00045494\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\z00045494\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\z00045494\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error: 0.154 (0.009)\n",
      "Wall time: 1.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.14934295, 0.16510608, 0.14850745, 0.14352604, 0.16436697])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_cv_scores(rf, X, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error: 0.125 (0.008)\n",
      "Wall time: 5.03 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.11448344, 0.13438681, 0.13498366, 0.1171377 , 0.12542057])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_cv_scores(gbr, X, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9581780008371052"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X, y_train)\n",
    "gbr.score(X, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_cv_scores(xgb_model, X, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_cv_scores(lgb_model, X, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Ensemble Techniques\n",
    "\n",
    "In this section, we will look at a few simple but powerful techniques, namely:\n",
    "1. Max Voting\n",
    "2. Averaging\n",
    "3. Weighted Averaging\n",
    "\n",
    "### Max Voting\n",
    "\n",
    "The **max voting** method is generally used for classification problems.\n",
    "\n",
    "In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a \"vote\". The predictions which we get from the majority of the models are used as the final prediction.\n",
    "\n",
    "For example, when you asked $5$ of your colleagues to rate your movie (out of $5$); we'll assume three of them rated it as $4$ while two of them gave it a $5$. Since the majority gave a rating of $4$, the final rating will be taken as $4$. You can consider this as taking the mode of all the predictions.\n",
    "\n",
    "The result of max voting would be something like this:\n",
    "\n",
    "| Colleague 1 | Colleague 2 | Colleague 3 | Colleague 4 | Colleague 5 |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 5 | 4 | 5 | 4 | 4 |\n",
    "\n",
    "Because the House Prices dataset is for regression, we take `scikit-learn`'s Iris Dataset for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 Score of the decision tree model: [1.         0.90909091 0.90909091]\n",
      "f1 Score of the KNN model: [1.         0.89361702 0.87804878]\n",
      "f1 Score of the LR model: [1.         0.97560976 0.9787234 ]\n",
      "f1 Score of the max voting model: [1.         0.93333333 0.93023256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z00045494\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\z00045494\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "iris = load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X, iris_y, test_size=0.5)\n",
    "\n",
    "# Models\n",
    "iris_dt_model = DecisionTreeClassifier()\n",
    "iris_knn_model = KNeighborsClassifier()\n",
    "iris_lr_model = LogisticRegression()\n",
    "\n",
    "# `fit()`\n",
    "iris_dt_model.fit(iris_X_train, iris_y_train)\n",
    "iris_knn_model.fit(iris_X_train, iris_y_train)\n",
    "iris_lr_model.fit(iris_X_train, iris_y_train)\n",
    "\n",
    "# `predict()`\n",
    "iris_dt_pred = iris_dt_model.predict(iris_X_test)\n",
    "iris_knn_pred = iris_knn_model.predict(iris_X_test)\n",
    "iris_lr_pred = iris_lr_model.predict(iris_X_test)\n",
    "\n",
    "# `f1_score()`\n",
    "print(\"f1 Score of the decision tree model:\", f1_score(iris_y_test, iris_dt_pred, average=None))\n",
    "print(\"f1 Score of the KNN model:\", f1_score(iris_y_test, iris_knn_pred, average=None))\n",
    "print(\"f1 Score of the LR model:\", f1_score(iris_y_test, iris_lr_pred, average=None))\n",
    "\n",
    "# Max voting\n",
    "iris_final_pred = np.array([])\n",
    "for i in range(0, len(iris_X_test)):\n",
    "    iris_final_pred = np.append(\n",
    "        iris_final_pred, \n",
    "        mode([iris_dt_pred[i], iris_knn_pred[i], iris_lr_pred[i]])\n",
    "    )\n",
    "    \n",
    "# F1 score of the ensemble model\n",
    "print(\"f1 Score of the max voting model:\", f1_score(iris_y_test, iris_final_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use `VotingClassifier` module in `sklearn`.\n",
    "\n",
    "If the `voting` argument is set `hard`, the classifier uses predicted class labels for majority rule voting. If it's set `soft`, the classifier predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 Score of the ensemble model: [1.         0.93333333 0.93023256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z00045494\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\z00045494\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "voting_model = VotingClassifier(estimators=[('lr', iris_lr_model), ('knn', iris_knn_model), ('dt', iris_dt_model)], voting='hard')\n",
    "voting_model.fit(iris_X_train, iris_y_train)\n",
    "voting_pred = voting_model.predict(iris_X_test)\n",
    "print(\"f1 Score of the ensemble model:\", f1_score(iris_y_test, voting_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging\n",
    "**Averaging** is a simple method that is generally used for regression problems. Here the predictions are simply averaged to get a more robust result and even though this method is simple it almost always gives better results than a single model and therefore is always worth trying.\n",
    "\n",
    "For ease of usage, we will create a class that inherits from `scikit-learn`'s `BaseEstimator`, `RegressorMixin`, and `TransformerMixin` classes because that way we only need to overwrite the fit and predict methods to get a working model which can be used like any other `scikit-learn` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can now be used like any other `scikit-learn` model. The only difference is that when creating an object we need to pass the base models which will then be trained in the `fit` method and used for predictions in the `prediction` method.\n",
    "\n",
    "By averaging three of our base models - Lasso regression, GBM, and XGBoost - we get a significant accuracy increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "averaged_model_1 = AveragedModels([gbr, lasso_model, xgb_model])\n",
    "get_cv_scores(averaged_model_1, X, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "averaged_model_2 = AveragedModels([gbr, lasso_model, xgb_model, lgb_model])\n",
    "get_cv_scores(averaged_model_2, X, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Average\n",
    "Averaging models is great and simple but it has one major flaw and that is that most of the time one model has more predictive power than another and therefore we want to give it more weight on the final predictions.\n",
    "We can achieve this by passing a weight for each of the models and then multiplying the predictions of each model with the corresponding weight. The only thing we need to look out for is that the weights need to add up to $1$ in order not to change the scale of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models, weights):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        assert sum(self.weights) == 1\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.sum(predictions*self.weights, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To now create the model we not only need to pass the base models but also an array of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weighted_average_model_1 = WeightedAveragedModels([gbr, lasso_model, xgb_model], [0.3, 0.3, 0.4])\n",
    "get_cv_scores(weighted_average_model_1, X, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weighted_average_model_2 = WeightedAveragedModels([gbr, lasso_model, xgb_model], [0.3, 0.45, 0.25])\n",
    "get_cv_scores(weighted_average_model_2, X, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Ensemble Techniques\n",
    "Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to **decrease variance** (bagging), **bias** (boosting), or **improve predictions** (stacking).\n",
    "\n",
    "Ensemble methods can be divided into two groups:\n",
    "\n",
    "* **Sequential** ensemble methods where the base learners are generated sequentially (e.g. AdaBoost). The basic motivation of sequential methods is to exploit the dependence between the base learners. The overall performance can be boosted by weighing previously mislabeled examples with higher weight.\n",
    "\n",
    "\n",
    "* **Parallel** ensemble methods where the base learners are generated in parallel (e.g. Random Forest). The basic motivation of parallel methods is to exploit independence between the base learners since the error can be reduced dramatically by averaging.\n",
    "\n",
    "Most ensemble methods use a single base learning algorithm to produce homogeneous base learners, i.e. learners of the same type, leading to homogeneous ensembles.\n",
    "\n",
    "There are also some methods that use heterogeneous learners, i.e. learners of different types, leading to heterogeneous ensembles. In order for ensemble methods to be more accurate than any of its individual members, the base learners have to be as accurate as possible and as diverse as possible.\n",
    "\n",
    "### Bagging\n",
    "\n",
    "**Bagging** stands for bootstrap aggregation. One way to reduce the variance of an estimate is to average together multiple estimates. For example, we can train M different trees on different subsets of the data (chosen randomly with replacement) and compute the ensemble:\n",
    "\n",
    "$f(x) = 1/M \\sum^{M}_{m=1} f_{m}(x)$\n",
    "\n",
    "Bagging uses bootstrap sampling to obtain the data subsets for training the base learners. For aggregating the outputs of base learners, bagging uses voting for classification and averaging for regression.\n",
    "\n",
    "![bagging-1](images/bagging-1.png)\n",
    "\n",
    "1. Multiple subsets are created from the original dataset, selecting observations with replacement.\n",
    "\n",
    "2. A base model (weak model) is created on each of these subsets.\n",
    "\n",
    "3. The models run in parallel and are independent of each other.\n",
    "\n",
    "4. The final predictions are determined by combining the predictions from all the models.\n",
    "\n",
    "![bagging-2](images/bagging-2.png)\n",
    "\n",
    "As each model is exposed to a different subset of data and we use their collective output at the end, so we are making sure that problem of overfitting is taken care of by not clinging too closely to our training data set. Thus, Bagging helps us to reduce the variance error.\n",
    "\n",
    "Combinations of multiple models decreases variance, especially in the case of unstable models, and may produce a more reliable prediction than a single model.\n",
    "\n",
    "In **random forests**, each tree in the ensemble is built from a sample drawn with replacement (i.e. a bootstrap sample) from the training set. In addition, instead of using all the features, a random subset of features is selected, further randomizing the tree.\n",
    "\n",
    "As a result, the bias of the forest increases slightly, but due to the averaging of less correlated trees, its variance decreases, resulting in an overall better model.\n",
    "\n",
    "In an **extremely randomized trees** algorithm randomness goes one step further: the splitting thresholds are randomized. Instead of looking for the most discriminative threshold, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows reduction of the variance of the model a bit more, at the expense of a slightly greater increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "\n",
    "        for model in self.models_:\n",
    "            X_tmp, y_tmp = self.subsample(X, y)\n",
    "            model.fit(X_tmp, y_tmp)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # Create a random subsample from the dataset with replacement\n",
    "    def subsample(self, X, y, ratio=1.0):\n",
    "        X_new, y_new = list(), list()\n",
    "        n_sample = round(len(X) * ratio)\n",
    "        while len(X_new) < n_sample:\n",
    "            index = np.random.randint(len(X))\n",
    "            X_new.append(X[index])\n",
    "            y_new.append(y[index])\n",
    "        return X_new, y_new\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this technique, we are able to create models which are highly uncorrelated and therefore perform really well when ensembled as can be seen when we compare a single decision tree and a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bagging_model = BaggingModels([gbr, lasso_model, xgb_model])\n",
    "get_cv_scores(bagging_model, X, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bagging_model = BaggingModels([gbr, lasso_model, xgb_model]*2)\n",
    "get_cv_scores(bagging_model, X, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "**Boosting** refers to a family of algorithms that are able to convert weak learners to strong learners. The main principle of boosting is to fit a sequence of weak learners - models that are only slightly better than random guessing, such as small decision trees - to weighted versions of the data. More weight is given to examples that were misclassified by earlier rounds.\n",
    "\n",
    "The predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. The principal difference between boosting and the committee methods, such as bagging, is that base learners are trained in sequence on a weighted version of the data.\n",
    "\n",
    "Boosting follows the following steps:\n",
    "1. Creates a subset from the original dataset (Initially, all datapoints are weighted equally).\n",
    "\n",
    "2. Creates and trains a base-model on the subset.\n",
    "\n",
    "3. The base model is used to make predictions on the whole dataset.\n",
    "\n",
    "![boosting-1](images/boosting-1.png)\n",
    "\n",
    "4. Errors of the predictions are calculated.\n",
    "\n",
    "5. Incorrectly predicted datapoints (datapoints with bigger error) are given higher weights.\n",
    "\n",
    "6. Another model is created on the datapoints with high weights.\n",
    "\n",
    "![boosting-2](images/boosting-2.png)\n",
    "\n",
    "7. Steps are repeated as long as needed.\n",
    "\n",
    "8. The final model is the weighted average of all models (better models get higher weights).\n",
    "\n",
    "![boosting-3](images/boosting-3.webp)\n",
    "\n",
    "Thus, the boosting algorithm combines a number of weak learners to form a strong learner. The individual models would not perform well on the entire dataset, but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble.\n",
    "\n",
    "![boosting-4](images/boosting-4.png)\n",
    "\n",
    "Boosting in general decreases the bias error and builds strong predictive models. Boosting has shown better predictive accuracy than bagging, but it also tends to over-fit the training data as well. Thus, parameter tuning becomes a crucial part of boosting algorithms to make them avoid overfitting.\n",
    "\n",
    "One of the first popular boosting implementations is called **AdaBoost**. To create an AdaBoost model we can simply use the `AdaBoostRegressor` model from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_model = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=3), n_estimators=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "**Stacking** is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on a complete training set, then the meta-model is trained on the outputs of the base level model as features. \n",
    "\n",
    "Simply put, stacking is an ensemble learning technique that uses predictions from multiple models (for example, decision tree, KNN or SVM) to build a new model. This model is used for making predictions on the test set.\n",
    "\n",
    "The base level often consists of different learning algorithms and therefore stacking ensembles are often heterogeneous.\n",
    "\n",
    "Below is a step-wise explanation for a simple stacked ensemble:\n",
    "\n",
    "1. The train set is split into $10$ parts.\n",
    "\n",
    "![stacking-1](images/stacking-1.png)\n",
    "\n",
    "2. A base model (suppose a decision tree) is fitted on $9$ parts and predictions are made for the 10th part. This is done for each part of the train set.\n",
    "\n",
    "![stacking-2](images/stacking-2.png)\n",
    "\n",
    "3. The base model (in this case, decision tree) is then fitted on the whole train dataset.\n",
    "\n",
    "4. Using this model, predictions are made on the test set.\n",
    "\n",
    "![stacking-3](images/stacking-3.png)\n",
    "\n",
    "5. Steps $2$ to $4$ are repeated for another base model (say KNN) resulting in another set of predictions for the train set and test set.\n",
    "\n",
    "![stacking-4](images/stacking-4.png)\n",
    "\n",
    "6. The predictions from the train set are used as features to build a new model.\n",
    "\n",
    "![stacking-5](images/stacking-5.png)\n",
    "\n",
    "7. This model is used to make final predictions on the test prediction set.\n",
    "\n",
    "Stacking was introduced by Wolpert in the paper [Stacked Generalization](https://www.researchgate.net/publication/222467943_Stacked_Generalization) in 1992. It is a method that uses $k$-fold for training base models which then make predictions on the left out fold. These so-called out of fold predictions are then used to train another model - the meta model - which can use the information produced by the base models to make final predictions.\n",
    "\n",
    "To implement this functionality we need to first train each base model $k$ times ($k...$ Number of folds) and then use their predictions to train our meta model.\n",
    "\n",
    "To even get better results we can not only use the predictions of all the base models for training the meta model but also the initial features. Because of the added model complexity which is caused when adding the input features we should make a boolean parameter to determine whether we want to use input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5, use_features_in_secondary=False):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "        self.use_features_in_secondary = use_features_in_secondary\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit all the models on the given dataset\"\"\"\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Train cloned base models and create out-of-fold predictions\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "        \n",
    "        if self.use_features_in_secondary:\n",
    "            self.meta_model_.fit(np.hstack((X, out_of_fold_predictions)), y)\n",
    "        else:\n",
    "            self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        if self.use_features_in_secondary:\n",
    "            return self.meta_model_.predict(np.hstack((X, meta_features)))\n",
    "        else:\n",
    "            return self.meta_model_.predict(meta_features)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        if self.use_features_in_secondary:\n",
    "            return self.meta_model_.predict_proba(np.hstack((X, meta_features)))\n",
    "        else:\n",
    "            return self.meta_model_.predict_proba(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stacking_model1 = StackingAveragedModels([gbr, lgb_model, xgb_model], lasso_model)\n",
    "get_cv_scores(stacking_model1, X, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stacking_model2= StackingAveragedModels([gbr, lgb_model, xgb_model], lasso_model, use_features_in_secondary=True)\n",
    "get_cv_scores(stacking_model2, X, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stacking_model3 = StackingAveragedModels([gbr, lgb_model, xgb_model, lasso_model], lasso_model)\n",
    "get_cv_scores(stacking_model3, X, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stacking_model4 = StackingAveragedModels([gbr, lgb_model, xgb_model, lasso_model], lasso_model, use_features_in_secondary=True)\n",
    "get_cv_scores(stacking_model4, X, y_train);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
